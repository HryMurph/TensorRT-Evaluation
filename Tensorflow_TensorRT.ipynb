{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow-TensorRT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IYChXgAlxTM"
      },
      "source": [
        "This notebook is the implementation of performance evaluation of Tensorflow-TensorRT. The first half runs with official weights from ImageNet, and the last half runs with models trained by myself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3DLaHjSHMus"
      },
      "source": [
        "# Restart the VM when the RAM is full\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfhmfRjpRscN"
      },
      "source": [
        "!pip install pillow matplotlib\n",
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY2KhASnSEeS"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version: \", tf.version.VERSION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0GwUTncSIMe"
      },
      "source": [
        "!wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "\n",
        "!dpkg -i nvidia-machine-learning-repo-*.deb\n",
        "!apt-get update\n",
        "\n",
        "!sudo apt-get install libnvinfer5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGX2JjrMSXl2"
      },
      "source": [
        "# check TensorRT version\n",
        "print(\"TensorRT version: \")\n",
        "!dpkg -l | grep nvinfer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88ZvhwzESdcD"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def check_tensor_core_gpu_present():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    for line in local_device_protos:\n",
        "        if \"compute capability\" in str(line):\n",
        "            compute_capability = float(line.physical_device_desc.split(\"compute capability: \")[-1])\n",
        "            if compute_capability>=7.0:\n",
        "                return True\n",
        "    \n",
        "print(\"Tensor Core GPU Present:\", check_tensor_core_gpu_present())\n",
        "tensor_core_gpu = check_tensor_core_gpu_present()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34_N8FI3SlVZ"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from tensorflow.keras.applications import densenet, inception_v3, mobilenet_v2, resnet50, vgg16, vgg19\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd5Qg8YISpny"
      },
      "source": [
        "%cd /content/\n",
        "!mkdir ./data\n",
        "!wget -O ./data/img0.JPG \"https://thumbs-prod.si-cdn.com/ej9KRK9frB5AXD6W9LXKFnuRc-0=/fit-in/1600x0/https://public-media.si-cdn.com/filer/ad/7b/ad7b3860-ad5f-43dc-800e-af57830cd1d3/labrador.jpg\"\n",
        "!wget -O ./data/img1.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHjtOYuK2n_CZoxQs9zxK96N1_qMiv3ZWSYQ&usqp=CAUg\"\n",
        "!wget -O ./data/img2.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRoEAt7d8PuZPBxWsjzvgQ_Y8Zfhgn1MvvA3Q&usqp=CAU\"\n",
        "!wget -O ./data/img3.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ9BZGaN2WhgsJJfLmEcEiwMRmgpSzJPjnacg&usqp=CAU\"\n",
        "!wget -O ./data/img4.JPG \"https://media.nature.com/lw800/magazine-assets/d41586-020-01430-5/d41586-020-01430-5_17977552.jpg\"\n",
        "!wget -O ./data/img5.JPG \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/golden-retriever-royalty-free-image-506756303-1560962726.jpg?crop=1.00xw:0.756xh;0,0.0756xh&resize=980:*\"\n",
        "!wget -O ./data/img6.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRH7_Z_Frxo_RbvJ6StY2TzQ0zFCgv6podjzw&usqp=CAU\"\n",
        "!wget -O ./data/img7.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4X0fwAtbfiSwRvN3-Fk1pE1rKMsAgWjcpbA&usqp=CAU\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWUqju6cSz9k"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "plot_row = 2\n",
        "plot_col = 4\n",
        "\n",
        "model_name = 'resnet'\n",
        "model = resnet50.ResNet50(weights='imagenet')\n",
        "size = 224\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
        "img_names = os.listdir('./data/')\n",
        "img_names = list(map(lambda x: './data/' + x, sorted(img_names)))\n",
        "\n",
        "for i, img_path in enumerate(img_names):\n",
        "  # img_path = './data/img%d.JPG'%i\n",
        "  img = image.load_img(img_path, target_size=(size, size))\n",
        "  plt.subplot(plot_row,plot_col,i+1)\n",
        "  plt.imshow(img);\n",
        "  plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTPBTYmPS5Rh"
      },
      "source": [
        "for i, img_path in enumerate(img_names):\n",
        "  # img_path = './data/img%d.JPG'%i\n",
        "  img = image.load_img(img_path, target_size=(size, size))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "\n",
        "  preds = model.predict(x)\n",
        "  # decode the results into a list of tuples (class, description, probability)\n",
        "  # (one such list for each sample in the batch)\n",
        "  print('{} - Predicted: {}'.format(img_path, decode_predictions(preds, top=3)[0]))\n",
        "\n",
        "  plt.subplot(plot_row,plot_col,i+1)\n",
        "  plt.imshow(img);\n",
        "  plt.axis('off');\n",
        "  plt.title(decode_predictions(preds, top=3)[0][0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIRvKGPyTe5P"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "model.save(model_name)\n",
        "# !saved_model_cli show --all --dir resnet50_saved_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC25bYwKUETf"
      },
      "source": [
        "model = tf.keras.models.load_model(model_name)\n",
        "img_path = './data/img0.JPG'  # Siberian_husky\n",
        "img = image.load_img(img_path, target_size=(size, size))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "# decode the results into a list of tuples (class, description, probability)\n",
        "# (one such list for each sample in the batch)\n",
        "print('{} - Predicted: {}'.format(img_path, decode_predictions(preds, top=3)[0]))\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(img);\n",
        "plt.axis('off');\n",
        "plt.title(decode_predictions(preds, top=3)[0][0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95hQI6e3HblN",
        "outputId": "28297bde-5bb3-4985-ca2a-f35bc798e075"
      },
      "source": [
        "batch_size = 8\n",
        "batched_input = np.zeros((batch_size, size, size, 3), dtype=np.float32)\n",
        "\n",
        "for i in range(batch_size):\n",
        "  img_path = './data/img%d.JPG' % (i % len(img_names))\n",
        "  img = image.load_img(img_path, target_size=(size, size))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  batched_input[i, :] = x\n",
        "batched_input = tf.constant(batched_input)\n",
        "print('batched_input shape: ', batched_input.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batched_input shape:  (8, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GogV7uXOUb3n"
      },
      "source": [
        "model = tf.keras.models.load_model(model_name)\n",
        "\n",
        "# Benchmarking throughput\n",
        "N_warmup_run = 50\n",
        "N_run = 1000\n",
        "elapsed_time = []\n",
        "\n",
        "for i in range(N_warmup_run):\n",
        "  preds = model.predict(batched_input)\n",
        "\n",
        "for i in range(N_run):\n",
        "  start_time = time.time()\n",
        "  preds = model.predict(batched_input)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "  if i % 50 == 0:\n",
        "    print('Step {}: {:.3f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
        "\n",
        "print('Throughput: {:.3f} ms/image'.format(elapsed_time.sum() * 1000 / (N_run * batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abNDyYuAgx5C"
      },
      "source": [
        "def predict_tftrt(input_saved_model):\n",
        "    \"\"\"Runs prediction on a single image and shows the result.\n",
        "    input_saved_model (string): Name of the input model stored in the current dir\n",
        "    \"\"\"\n",
        "    img_path = './data/img0.JPG'  # Siberian_husky\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    x = tf.constant(x)\n",
        "    \n",
        "    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n",
        "    signature_keys = list(saved_model_loaded.signatures.keys())\n",
        "    print(signature_keys)\n",
        "\n",
        "    infer = saved_model_loaded.signatures['serving_default']\n",
        "    print(infer.structured_outputs)\n",
        "\n",
        "    labeling = infer(x)\n",
        "    preds = labeling['probs'].numpy()\n",
        "    print('{} - Predicted: {}'.format(img_path, decode_predictions(preds, top=3)[0]))\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.imshow(img);\n",
        "    plt.axis('off');\n",
        "    plt.title(decode_predictions(preds, top=3)[0][0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRa0Y5vtg6hq"
      },
      "source": [
        "def benchmark_tftrt(input_saved_model):\n",
        "    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n",
        "    infer = saved_model_loaded.signatures['serving_default']\n",
        "\n",
        "    N_warmup_run = 50\n",
        "    N_run = 1000\n",
        "    elapsed_time = []\n",
        "\n",
        "    for i in range(N_warmup_run):\n",
        "      labeling = infer(batched_input)\n",
        "\n",
        "    for i in range(N_run):\n",
        "      start_time = time.time()\n",
        "      labeling = infer(batched_input)\n",
        "      #prob = labeling['probs'].numpy()\n",
        "      end_time = time.time()\n",
        "      elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "      if i % 50 == 0:\n",
        "        print('Step {}: {:.3f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
        "\n",
        "    print('Throughput: {:.3f} ms/image'.format(elapsed_time.sum() * 1000 / (N_run * batch_size)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twB4keHrTwAz"
      },
      "source": [
        "print('Converting to TF-TRT FP32...')\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n",
        "                                                               max_workspace_size_bytes=8000000000)\n",
        "\n",
        "converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_name,\n",
        "                                    conversion_params=conversion_params)\n",
        "converter.convert()\n",
        "converter.save(output_saved_model_dir=model_name+'_TFTRT_FP32')\n",
        "print('Done Converting to TF-TRT FP32')\n",
        "benchmark_tftrt(model_name+'_TFTRT_FP32')\n",
        "\n",
        "print('Converting to TF-TRT FP16...')\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "    precision_mode=trt.TrtPrecisionMode.FP16,\n",
        "    max_workspace_size_bytes=8000000000)\n",
        "converter = trt.TrtGraphConverterV2(\n",
        "   input_saved_model_dir=model_name, conversion_params=conversion_params)\n",
        "converter.convert()\n",
        "converter.save(output_saved_model_dir=model_name+'_TFTRT_FP16')\n",
        "print('Done Converting to TF-TRT FP16')\n",
        "benchmark_tftrt(model_name+'_TFTRT_FP16')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAnZuqiDR6Yc"
      },
      "source": [
        "print('Converting to TF-TRT INT8...')\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "    precision_mode=trt.TrtPrecisionMode.INT8, \n",
        "    max_workspace_size_bytes=8000000000, \n",
        "    use_calibration=True)\n",
        "converter = trt.TrtGraphConverterV2(\n",
        "    input_saved_model_dir=model_name, \n",
        "    conversion_params=conversion_params)\n",
        "def calibration_input_fn():\n",
        "    yield (batched_input, )\n",
        "converter.convert(calibration_input_fn=calibration_input_fn)\n",
        "converter.save(output_saved_model_dir=model_name+'_TFTRT_INT8')\n",
        "print('Done Converting to TF-TRT INT8')\n",
        "benchmark_tftrt(model_name+'_TFTRT_INT8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dugxzSspljpm"
      },
      "source": [
        "Evaluate performance using models trained by myself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk8Qd9ZeRM1E"
      },
      "source": [
        "model = resnet50.ResNet50(\n",
        "    weights=None,\n",
        "    classes=10\n",
        ")\n",
        "size = 224\n",
        "model_name = 'resnet_mnist'\n",
        "(train_image,train_label),(test_image,test_label)=tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps9BfS3yG8LP"
      },
      "source": [
        "import cv2 as cv\n",
        "\n",
        "train_data = []\n",
        "for img in train_image[:500]:\n",
        "    resized_img = cv.resize(img, (size, size))\n",
        "    resized_img = cv.cvtColor(resized_img, cv.COLOR_GRAY2BGR)\n",
        "    train_data.append(resized_img)\n",
        "\n",
        "train_data=np.array(train_data)\n",
        "print(train_data.shape)\n",
        "\n",
        "train_data=train_data/255\n",
        "\n",
        "model.compile(optimizer=\"Adam\",\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_data,train_label[0:500], epochs=50, batch_size=6)\n",
        "model.save(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LuGmyJRRUCv",
        "outputId": "9bc81f3d-8ed9-421c-88fa-ca73fb64a957"
      },
      "source": [
        "import cv2 as cv\n",
        "\n",
        "model = tf.keras.models.load_model(model_name)\n",
        "batch_size = 8\n",
        "batched_input = np.zeros((batch_size, size, size, 3), dtype=np.float32)\n",
        "\n",
        "test_data = []\n",
        "# for i in range(batch_size):\n",
        "for img in test_image[:batch_size]:\n",
        "  resized_img = cv.resize(img, (size, size))\n",
        "  resized_img = cv.cvtColor(resized_img, cv.COLOR_GRAY2BGR)\n",
        "  test_data.append(resized_img)\n",
        "\n",
        "test_data=np.array(test_data)\n",
        "test_data=test_data/255.0\n",
        "\n",
        "batched_input = tf.constant(test_data, dtype=tf.float32)\n",
        "print('batched_input shape: ', batched_input.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batched_input shape:  (8, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "323s-pKeLD7M"
      },
      "source": [
        "# Benchmarking throughput\n",
        "N_warmup_run = 50\n",
        "N_run = 1000\n",
        "elapsed_time = []\n",
        "\n",
        "for i in range(N_warmup_run):\n",
        "  preds = model.predict(batched_input)\n",
        "\n",
        "for i in range(N_run):\n",
        "  start_time = time.time()\n",
        "  preds = model.predict(batched_input)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "  if i % 50 == 0:\n",
        "    print('Step {}: {:.3f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
        "\n",
        "print('Throughput: {:.3f} ms/image'.format(elapsed_time.sum() * 1000 / (N_run * batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9vXUtXSMxPn"
      },
      "source": [
        "def benchmark_tftrt(input_saved_model):\n",
        "    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n",
        "    infer = saved_model_loaded.signatures['serving_default']\n",
        "    print('infer:', infer)\n",
        "\n",
        "    N_warmup_run = 50\n",
        "    N_run = 1000\n",
        "    elapsed_time = []\n",
        "\n",
        "    for i in range(N_warmup_run):\n",
        "      labeling = infer(batched_input)\n",
        "\n",
        "    for i in range(N_run):\n",
        "      start_time = time.time()\n",
        "      labeling = infer(batched_input)\n",
        "      #prob = labeling['probs'].numpy()\n",
        "      end_time = time.time()\n",
        "      elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "      if i % 50 == 0:\n",
        "        print('Step {}: {:.3f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
        "\n",
        "    print('Throughput: {:.3f} ms/image'.format(elapsed_time.sum() * 1000 / (N_run * batch_size)))\n",
        "\n",
        "print('Converting to TF-TRT FP32...')\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n",
        "                                                               max_workspace_size_bytes=8000000000)\n",
        "\n",
        "converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_name,\n",
        "                                    conversion_params=conversion_params)\n",
        "converter.convert()\n",
        "converter.save(output_saved_model_dir=model_name+'_TFTRT_FP32')\n",
        "print('Done Converting to TF-TRT FP32')\n",
        "benchmark_tftrt(model_name+'_TFTRT_FP32')\n",
        "\n",
        "print('Converting to TF-TRT FP16...')\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "    precision_mode=trt.TrtPrecisionMode.FP16,\n",
        "    max_workspace_size_bytes=8000000000)\n",
        "converter = trt.TrtGraphConverterV2(\n",
        "   input_saved_model_dir=model_name, conversion_params=conversion_params)\n",
        "converter.convert()\n",
        "converter.save(output_saved_model_dir=model_name+'_TFTRT_FP16')\n",
        "print('Done Converting to TF-TRT FP16')\n",
        "benchmark_tftrt(model_name+'_TFTRT_FP16')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbKDs_99R8-d"
      },
      "source": [
        "def benchmark_tftrt(input_saved_model):\n",
        "    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n",
        "    infer = saved_model_loaded.signatures['serving_default']\n",
        "    print('infer:', infer)\n",
        "\n",
        "    N_warmup_run = 50\n",
        "    N_run = 1000\n",
        "    elapsed_time = []\n",
        "\n",
        "    for i in range(N_warmup_run):\n",
        "      labeling = infer(batched_input)\n",
        "\n",
        "    for i in range(N_run):\n",
        "      start_time = time.time()\n",
        "      labeling = infer(batched_input)\n",
        "      #prob = labeling['probs'].numpy()\n",
        "      end_time = time.time()\n",
        "      elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "      if i % 50 == 0:\n",
        "        print('Step {}: {:.3f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
        "\n",
        "    print('Throughput: {:.3f} ms/image'.format(elapsed_time.sum() * 1000 / (N_run * batch_size)))\n",
        "\n",
        "print('Converting to TF-TRT INT8...')\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "    precision_mode=trt.TrtPrecisionMode.INT8, \n",
        "    max_workspace_size_bytes=8000000000, \n",
        "    use_calibration=True)\n",
        "converter = trt.TrtGraphConverterV2(\n",
        "    input_saved_model_dir=model_name, \n",
        "    conversion_params=conversion_params)\n",
        "def calibration_input_fn():\n",
        "    yield (batched_input, )\n",
        "converter.convert(calibration_input_fn=calibration_input_fn)\n",
        "converter.save(output_saved_model_dir=model_name+'_TFTRT_INT8')\n",
        "print('Done Converting to TF-TRT INT8')\n",
        "benchmark_tftrt(model_name+'_TFTRT_INT8')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}