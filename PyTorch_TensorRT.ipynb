{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-TensorRT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWuL_aN4nWca"
      },
      "source": [
        "This file is the implementation of example and performance evaluation of PyTorch-TensorRT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzwpVMgRIF_4"
      },
      "source": [
        "# Link your Google Drive to /content/drive/\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p /content/drive\n",
        "!google-drive-ocamlfuse /content/drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moq5XJEUJFBH"
      },
      "source": [
        "# Check python version and cuda version\n",
        "!python --version\n",
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT1oHcvQI-fm"
      },
      "source": [
        "# Install TensorRT and other requirements\n",
        "# You need to download the DEB file and TAR file of TensorRT in correct version firstly and upload them to your Google Drive\n",
        "!dpkg -i /content/drive/nv-tensorrt-repo-ubuntu1804-cuda11.0-trt7.2.3.4-ga-20210226_1-1_amd64.deb\n",
        "\n",
        "!apt-key add /var/nv-tensorrt-repo-cuda11.0-trt7.2.3.4-ga-20210226/7fa2af80.pub\n",
        "\n",
        "!apt-get update\n",
        "\n",
        "!apt-get install libnvinfer7=7.2.3-1+cuda11.0 libnvonnxparsers7=7.2.3-1+cuda11.0 libnvparsers7=7.2.3-1+cuda11.0 libnvinfer-plugin7=7.2.3-1+cuda11.0 libnvinfer-dev=7.2.3-1+cuda11.0 libnvonnxparsers-dev=7.2.3-1+cuda11.0 libnvparsers-dev=7.2.3-1+cuda11.0 libnvinfer-plugin-dev=7.2.3-1+cuda11.0 python-libnvinfer=7.2.3-1+cuda11.0 python3-libnvinfer=7.2.3-1+cuda11.0\n",
        "\n",
        "!apt-mark hold libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev python-libnvinfer python3-libnvinfer\n",
        "\n",
        "!apt-get install tensorrt\n",
        "!pip install pycuda\n",
        "!pip install tensorflow-gpu==2.4.1\n",
        "\n",
        "%cd /content\n",
        "!cp /content/drive/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz\n",
        "!tar -zxvf TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz\n",
        "%cd /content/TensorRT-7.2.3.4/python\n",
        "!pip install tensorrt-7.2.3.4-cp37-none-linux_x86_64.whl\n",
        "%cd /content/TensorRT-7.2.3.4/graphsurgeon\n",
        "!pip install graphsurgeon-0.4.5-py2.py3-none-any.whl\n",
        "%cd /content/TensorRT-7.2.3.4/uff\n",
        "!pip install uff-0.6.9-py2.py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfWSUsXLJ79D"
      },
      "source": [
        "# Check if the installation of TensorRT is success\n",
        "import tensorrt as trt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1oDM76PLIQg"
      },
      "source": [
        "onnx_file_path = 'mobilenet.onnx'\n",
        "engine_file_path = 'mobilenet.trt'\n",
        "verbose = False\n",
        "baseline = True  # whether do the inference with original PyTorch or not\n",
        "input_image_path = 'img0.JPG'"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2GAMjYplJPg"
      },
      "source": [
        "%cd /content\n",
        "!wget -O img0.JPG \"https://thumbs-prod.si-cdn.com/ej9KRK9frB5AXD6W9LXKFnuRc-0=/fit-in/1600x0/https://public-media.si-cdn.com/filer/ad/7b/ad7b3860-ad5f-43dc-800e-af57830cd1d3/labrador.jpg\"\n",
        "!wget -O img1.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHjtOYuK2n_CZoxQs9zxK96N1_qMiv3ZWSYQ&usqp=CAUg\"\n",
        "!wget -O img2.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRoEAt7d8PuZPBxWsjzvgQ_Y8Zfhgn1MvvA3Q&usqp=CAU\"\n",
        "!wget -O img3.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ9BZGaN2WhgsJJfLmEcEiwMRmgpSzJPjnacg&usqp=CAU\"\n",
        "!wget -O img4.JPG \"https://media.nature.com/lw800/magazine-assets/d41586-020-01430-5/d41586-020-01430-5_17977552.jpg\"\n",
        "!wget -O img5.JPG \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/golden-retriever-royalty-free-image-506756303-1560962726.jpg?crop=1.00xw:0.756xh;0,0.0756xh&resize=980:*\"\n",
        "!wget -O img6.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRH7_Z_Frxo_RbvJ6StY2TzQ0zFCgv6podjzw&usqp=CAU\"\n",
        "!wget -O img7.JPG \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4X0fwAtbfiSwRvN3-Fk1pE1rKMsAgWjcpbA&usqp=CAU\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz6_9NBcoaGN"
      },
      "source": [
        "# Example code\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import argparse\n",
        "import tensorrt as trt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pycuda\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "\n",
        "def onnx(onnx_file_path, verbose):\n",
        "  if not os.path.exists(onnx_file_path):\n",
        "    print(\"Generating ONNX file for MobileNet_V2: \", onnx_file_path)\n",
        "    dummy_input = torch.randn(1, 3, 224, 224, device='cuda')\n",
        "    model = torchvision.models.mobilenet_v2(pretrained=True).cuda()\n",
        "    input_names = [\"actual_input_1\"] + [\"learned_%d\" % i for i in range(16)]\n",
        "    output_names = [\"output1\"]\n",
        "    torch.onnx.export(model, dummy_input, onnx_file_path, verbose=verbose, input_names=input_names, output_names=output_names)\n",
        "\n",
        "  print(\"Loading ONNX file from: \", onnx_file_path)\n",
        "  onnx_model = open(onnx_file_path, 'rb')\n",
        "  return onnx_model\n",
        "\n",
        "def trt_engine(engine_file_path, onnx_model):\n",
        "\tmodel_engine = None\n",
        "\tif os.path.exists(engine_file_path):\n",
        "\t\tprint(\"Reading engine from: \", engine_file_path)\n",
        "\t\t# deserialize the engine file\n",
        "\t\twith open(engine_file_path, \"rb\") as model, trt.Runtime(TRT_LOGGER) as runtime:\n",
        "\t\t\tmodel_engine = runtime.deserialize_cuda_engine(model.read())\n",
        "\telse:\n",
        "\t\twith trt.Builder(TRT_LOGGER) as builder:\n",
        "\t\t\t# Specify that the network should be created with an explicit batch dimension\n",
        "\t\t\tEXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "\t\t\tnetwork = builder.create_network(EXPLICIT_BATCH)\n",
        "\t\t\tparser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "\t\t\tbuilder.max_workspace_size = 1 << 28\n",
        "\t\t\tbuilder.max_batch_size = 1\n",
        "\t\t\tparser.parse(onnx_model.read())\n",
        "\t\t\tmodel_engine = builder.build_cuda_engine(network)\n",
        "\t\t\twith open(engine_file_path, \"wb\") as f:\n",
        "\t\t\t\tf.write(model_engine.serialize())\n",
        "\treturn model_engine\n",
        "\n",
        "def get_image(input_image_path):\n",
        "\tprint(\"Get image: \", input_image_path)\n",
        "\timage = Image.open(input_image_path)\n",
        "\tprint(\"Input image format {}, size {}, mode {}.\".format(image.format, image.size, image.mode))\n",
        "\tpreprocess = transforms.Compose([\n",
        "\t\ttransforms.Resize(256),\n",
        "\t\ttransforms.CenterCrop(224),\n",
        "\t\ttransforms.ToTensor(),\n",
        "\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\t])\n",
        "\timage = preprocess(image)\n",
        "\tprint(\"Image size after preprocessing: \", image.shape)\n",
        "\timage_binary = np.array(image, dtype=np.float32, order='C')\n",
        "\treturn image_binary\n",
        "\n",
        "def allocate_buffers(model_engine):\n",
        "\tbindings \t= []\n",
        "\tinputs \t\t= []\n",
        "\toutputs \t= []\n",
        "\t# binding: describe the input and output ports of the engine\n",
        "\tfor binding in model_engine:\n",
        "\t\tdata_size \t\t= trt.volume(model_engine.get_binding_shape(binding)) * model_engine.max_batch_size\n",
        "\t\tdata_type \t\t= trt.nptype(model_engine.get_binding_dtype(binding))\n",
        "\t\thost_memory \t= pycuda.driver.pagelocked_empty(data_size, data_type)\n",
        "\t\tdevice_memory \t= pycuda.driver.mem_alloc(host_memory.nbytes)\n",
        "\t\t# stored the memory index in CUDA context\n",
        "\t\tbindings.append(int(device_memory))\n",
        "\t\tif model_engine.binding_is_input(binding):\n",
        "\t\t\tinputs.append({\"host\": host_memory, \"device\": device_memory})\n",
        "\t\telse:\n",
        "\t\t\toutputs.append({\"host\": host_memory, \"device\": device_memory})\n",
        "\treturn inputs, outputs, bindings\n",
        "\n",
        "def do_inference(context, bindings, inputs, outputs, stream):\n",
        "\t# send inputs to device (GPU)\n",
        "\tfor input in inputs:\n",
        "\t\tpycuda.driver.memcpy_htod_async(input[\"device\"], input[\"host\"], stream)\n",
        "\t# do inference\n",
        "\tstart = time.clock()\n",
        "\tcontext.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "\tend = time.clock()\n",
        "\t# send outputs to host (CPU)\n",
        "\tfor output in outputs:\n",
        "\t\tpycuda.driver.memcpy_dtoh_async(output[\"host\"], output[\"device\"], stream)\n",
        "\t# waot for all activity on this stream to cease, then return.\n",
        "\tstream.synchronize()\n",
        "\treturn [output[\"host\"] for output in outputs], (end-start)*1000\n",
        "\n",
        "def post_process(outputs):\n",
        "\toutput = torch.Tensor(outputs[0])\n",
        "\treturn torch.nn.functional.softmax(output, dim=0).argmax(dim=0)\n",
        "\n",
        "def pytorch_baseline(input_image_path):\n",
        "\tmodel = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=True)\n",
        "\tmodel.eval()\n",
        "\tinput_image = Image.open(input_image_path)\n",
        "\tprint(\"Input image format {}, size {}, mode {}.\".format(input_image.format, input_image.size, input_image.mode))\n",
        "\tpreprocess = transforms.Compose([\n",
        "\t\ttransforms.Resize(256),\n",
        "\t\ttransforms.CenterCrop(224),\n",
        "\t\ttransforms.ToTensor(),\n",
        "\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\t])\n",
        "\tinput_tensor = preprocess(input_image)\n",
        "\tinput_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tinput_batch = input_batch.to('cuda')\n",
        "\t\tmodel.to('cuda')\n",
        "\toutput = None\n",
        "\tstart = time.clock()\n",
        "\twith torch.no_grad():\n",
        "\t\toutput = model(input_batch)\n",
        "\tend = time.clock()\n",
        "\n",
        "\treturn output, (end-start)*1000\n",
        "\n",
        "# Get model onnx file\n",
        "onnx_model = onnx(onnx_file_path, verbose)\n",
        "# Build cuda engine\n",
        "model_engine = trt_engine(engine_file_path, onnx_model)\n",
        "# Prepare inputs\n",
        "image_binary = get_image(input_image_path)\n",
        "\n",
        "# Create cuda context\n",
        "trt_output = None\n",
        "trt_time = 0\n",
        "with model_engine.create_execution_context() as context:\n",
        "  inputs, outputs, bindings = allocate_buffers(model_engine)\n",
        "  # A handle for a queue of operations that will be carried out in order.\n",
        "  stream = pycuda.driver.Stream()\n",
        "  inputs[0][\"host\"] = image_binary\n",
        "  # Do inference\n",
        "  outputs, trt_time = do_inference(context, bindings, inputs, outputs, stream)\n",
        "  # Process outputs\n",
        "  trt_output = torch.nn.functional.softmax(torch.Tensor(outputs[0]), dim=0)\n",
        "  print(\"trt_label:   \", trt_output.argmax(dim=0).numpy())\n",
        "  print(\"trt_time:     %.3f ms.\" % trt_time)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "if baseline:\n",
        "  pth_output, pth_time = pytorch_baseline(input_image_path)\n",
        "  pth_output = torch.nn.functional.softmax(pth_output[0], dim=0).cpu()\n",
        "  pth_label = pth_output.argmax(dim=0).item()\n",
        "  print(\"pth_label:\", pth_label)\n",
        "  print(\"pth_time: %.3f ms.\" % pth_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bjll2wLKJIN"
      },
      "source": [
        "# Modified code for benchmark\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import argparse\n",
        "import tensorrt as trt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pycuda\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "\n",
        "def onnx(onnx_file_path, verbose):\n",
        "  if not os.path.exists(onnx_file_path):\n",
        "    print(\"Generating ONNX file for MobileNet_V2: \", onnx_file_path)\n",
        "    dummy_input = torch.randn(1, 3, 224, 224, device='cuda')\n",
        "    model = torchvision.models.mobilenet_v2(pretrained=True).cuda()\n",
        "    input_names = [\"actual_input_1\"] + [\"learned_%d\" % i for i in range(16)]\n",
        "    output_names = [\"output1\"]\n",
        "    torch.onnx.export(model, dummy_input, onnx_file_path, verbose=verbose, input_names=input_names, output_names=output_names)\n",
        "\n",
        "  print(\"Loading ONNX file from: \", onnx_file_path)\n",
        "  onnx_model = open(onnx_file_path, 'rb')\n",
        "  return onnx_model\n",
        "\n",
        "def trt_engine(engine_file_path, onnx_model):\n",
        "\tmodel_engine = None\n",
        "\tif os.path.exists(engine_file_path):\n",
        "\t\tprint(\"Reading engine from: \", engine_file_path)\n",
        "\t\t# deserialize the engine file\n",
        "\t\twith open(engine_file_path, \"rb\") as model, trt.Runtime(TRT_LOGGER) as runtime:\n",
        "\t\t\tmodel_engine = runtime.deserialize_cuda_engine(model.read())\n",
        "\telse:\n",
        "\t\twith trt.Builder(TRT_LOGGER) as builder:\n",
        "\t\t\t# Specify that the network should be created with an explicit batch dimension\n",
        "\t\t\tEXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "\t\t\tnetwork = builder.create_network(EXPLICIT_BATCH)\n",
        "\t\t\tparser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "\t\t\tbuilder.max_workspace_size = 1 << 28\n",
        "\t\t\tbuilder.max_batch_size = 1\n",
        "\t\t\tparser.parse(onnx_model.read())\n",
        "\t\t\tmodel_engine = builder.build_cuda_engine(network)\n",
        "\t\t\twith open(engine_file_path, \"wb\") as f:\n",
        "\t\t\t\tf.write(model_engine.serialize())\n",
        "\treturn model_engine\n",
        "\n",
        "def get_image(input_image_path):\n",
        "\tprint(\"Get image: \", input_image_path)\n",
        "\timage = Image.open(input_image_path)\n",
        "\tprint(\"Input image format {}, size {}, mode {}.\".format(image.format, image.size, image.mode))\n",
        "\tpreprocess = transforms.Compose([\n",
        "\t\ttransforms.Resize(256),\n",
        "\t\ttransforms.CenterCrop(224),\n",
        "\t\ttransforms.ToTensor(),\n",
        "\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\t])\n",
        "\timage = preprocess(image)\n",
        "\tprint(\"Image size after preprocessing: \", image.shape)\n",
        "\timage_binary = np.array(image, dtype=np.float32, order='C')\n",
        "\treturn image_binary\n",
        "\n",
        "def allocate_buffers(model_engine):\n",
        "\tbindings \t= []\n",
        "\tinputs \t\t= []\n",
        "\toutputs \t= []\n",
        "\t# binding: describe the input and output ports of the engine\n",
        "\tfor binding in model_engine:\n",
        "\t\tdata_size \t\t= trt.volume(model_engine.get_binding_shape(binding)) * model_engine.max_batch_size\n",
        "\t\tdata_type \t\t= trt.nptype(model_engine.get_binding_dtype(binding))\n",
        "\t\thost_memory \t= pycuda.driver.pagelocked_empty(data_size, data_type)\n",
        "\t\tdevice_memory \t= pycuda.driver.mem_alloc(host_memory.nbytes)\n",
        "\t\t# stored the memory index in CUDA context\n",
        "\t\tbindings.append(int(device_memory))\n",
        "\t\tif model_engine.binding_is_input(binding):\n",
        "\t\t\tinputs.append({\"host\": host_memory, \"device\": device_memory})\n",
        "\t\telse:\n",
        "\t\t\toutputs.append({\"host\": host_memory, \"device\": device_memory})\n",
        "\treturn inputs, outputs, bindings\n",
        "\n",
        "def do_inference(context, bindings, inputs, outputs, stream):\n",
        "\t# send inputs to device (GPU)\n",
        "\tfor input in inputs:\n",
        "\t\tpycuda.driver.memcpy_htod_async(input[\"device\"], input[\"host\"], stream)\n",
        "\t# do inference\n",
        "\tstart = time.clock()\n",
        "\tcontext.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "\tend = time.clock()\n",
        "\t# send outputs to host (CPU)\n",
        "\tfor output in outputs:\n",
        "\t\tpycuda.driver.memcpy_dtoh_async(output[\"host\"], output[\"device\"], stream)\n",
        "\t# waot for all activity on this stream to cease, then return.\n",
        "\tstream.synchronize()\n",
        "\treturn [output[\"host\"] for output in outputs], (end-start)*1000\n",
        "\n",
        "def post_process(outputs):\n",
        "\toutput = torch.Tensor(outputs[0])\n",
        "\treturn torch.nn.functional.softmax(output, dim=0).argmax(dim=0)\n",
        "\n",
        "def pytorch_baseline(model, input_batch):\n",
        "  batch_time = 0\n",
        "  for i in range(8):\n",
        "    output = None\n",
        "    start = time.clock()\n",
        "    with torch.no_grad():\n",
        "      output = model(input_batch[i])\n",
        "    end = time.clock()\n",
        "    batch_time += (end-start)*1000\n",
        "  return output, batch_time\n",
        "\n",
        "\n",
        "# Get model onnx file\n",
        "onnx_model = onnx(onnx_file_path, verbose)\n",
        "# Build cuda engine\n",
        "model_engine = trt_engine(engine_file_path, onnx_model)\n",
        "# Prepare inputs\n",
        "images_binary = []\n",
        "for i in range(8):\n",
        "  images_binary.append(get_image('img'+str(i)+'.JPG'))\n",
        "\n",
        "# Create cuda context\n",
        "trt_output = None\n",
        "trt_time = 0\n",
        "trt_time_total = 0\n",
        "for _ in range(1000):\n",
        "  for i in range(8):\n",
        "    with model_engine.create_execution_context() as context:\n",
        "      inputs, outputs, bindings = allocate_buffers(model_engine)\n",
        "      # A handle for a queue of operations that will be carried out in order.\n",
        "      stream = pycuda.driver.Stream()\n",
        "      inputs[0][\"host\"] = images_binary[i]\n",
        "      # Do inference\n",
        "      outputs, trt_time = do_inference(context, bindings, inputs, outputs, stream)\n",
        "      trt_time_total += trt_time\n",
        "print(\"avg_trt_time: %.3f ms/image\" % (trt_time_total/(8*1000)))\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if baseline:\n",
        "  pth_time_total = 0\n",
        "  model = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=True)\n",
        "  model.eval()\n",
        "  input_batch = []\n",
        "  for i in range(8):\n",
        "    input_image = Image.open('img'+str(i)+'.JPG')\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    input_tensor = preprocess(input_image)\n",
        "    input_batch.append(input_tensor.unsqueeze(0))\n",
        "  if torch.cuda.is_available():\n",
        "    for i in range(8):\n",
        "      input_batch[i] = input_batch[i].to('cuda')\n",
        "      model.to('cuda')\n",
        "  for _ in range(1000):\n",
        "    pth_output, pth_time = pytorch_baseline(model, input_batch)\n",
        "    pth_time_total += pth_time\n",
        "  print(\"avg_pth_time: %.3f ms/image\" % (pth_time_total/(1000*8)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}